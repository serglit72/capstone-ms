version: 2.1
orbs:
  aws-cli: circleci/aws-cli@3.1.1
  commands:
    print_pipeline_id:
      description: "Print a CircleCI workflowID"
      steps:
        - run: echo ${CIRCLE_WORKFLOW_ID}
    
    destroy_environment:
      steps:
        - run:
            name: Destroy environment
            command: |
              aws cloudformation delete-stack --stack-name prod-${CIRCLE_WORKFLOW_ID}


jobs:
  build-env:
    docker:
    - image: cimg/python:3.9
      auth:
        username: $DOCKERHUB_LOGIN
        password: $DOCKERHUB_PASSWORD # context / project UI env-var reference

    steps:
    - checkout
    - run:
        name: "Install Dependencies"
        command: |
          sudo apt update
          python3 -m venv capstone/venv

          . capstone/venv/bin/activate
          pip install --upgrade pip
          pip install -r requirements.txt

    - persist_to_workspace:
        # Must be an absolute path, or relative path from working_directory. This is a directory on the container which is
        # taken to be the root directory of the workspace.
        root: ~/project
        # Must be relative path from root
        paths:
        - capstone/venv
        - ./
    - save_cache:
        key: v1-dependencies-{{ checksum "requirements.txt" }}
        paths:
        - ~/project

  test-build:
    docker:
    - image: cimg/python:3.9
    steps:
    - attach_workspace:
        at: ~/project

    - run:
        name: "lint and pytest: index page and status page"
        command: ". capstone/venv/bin/activate\nwget -O hadolint https://github.com/hadolint/hadolint/releases/download/v2.12.0/hadolint-Linux-x86_64 &&\\\nchmod +x hadolint\n./hadolint Dockerfile\npylint --disable=R,C,W1203,W1202 api.py\n# pytest \npython3 -m pytest -vv ./tests/\n#magic___^_^___line\n#magic___^_^___line\n"
  docker-build-push:
    docker:
    - image: cimg/base:2022.06
    steps:
    - checkout
    - restore_cache:
        keys:
        - v1-dependencies-{{ checksum "requirements.txt" }}
        # fallback to using the latest cache if no exact match is found
        - v1-dependencies-
    - setup_remote_docker:
        docker_layer_caching: true
    - run:
        name: "Build and push Docker image to Dockerhub"
        command: |
          echo "$DOCKERHUB_PASSWORD" | docker login --username $DOCKERHUB_LOGIN --password-stdin
          # docker rmi $(docker images -a -q)
          docker build -t serglit72/flask-app:latest . && docker images
          docker push serglit72/flask-app:latest

  create-vpc-for-kube-cluster:
    docker:
    - image: cimg/aws:2022.11
    steps:
    - checkout
    - run:
        name: "Create VPC for EKS cluster"
        command: | 
          aws cloudformation create-stack \
          --stack-name eks-vpc-${CIRCLE_WORKFLOW_ID}\
          --template-body /eks/amazon-eks-vpc-private-subnets.yaml \
          --region=us-west-1

    - destroy_environment:
          when: on_fail


  infrastructure:

    docker:
    - image: cimg/aws:2022.11
    steps:
    - checkout

    - run:
        name: "Install eksctl"
        command: |

          curl --location "https://github.com/weaveworks/eksctl/releases/download/v0.121.0/eksctl_Linux_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          ls -la /usr/local/bin
# Our cluster consists two node groups
# Small on-demand node group ‘ng-on-demand’, for ???Jenkins??? master, also you can put some monitoring tools and Vault if needed. 
# If it’s only for Jenkins you can provision only one regular on-demand EC2 instance. 
# We want our Jenkins master to always be available that why we not using spot instances in this case.
# Dynamic node group ‘ng-spot’ of size ‘large’ with minSize of 0 and maxSize of 10, and desiredCapacity of 0, 
# based on spot instances and scaled by ‘Cluster Autoscaler’ to needed size according to load.
#


    - run:
        name: "Create kubernetes cluster on VPC "
        command: |
          eksctl create cluster -f eks/eks-cluster-spots.yaml

  infrastructure-check:
    docker:
    - image: cimg/aws:2022.11
    steps:
    - checkout
    - run:
        name: "Install eksctl"
        command: |
          curl --location "https://github.com/weaveworks/eksctl/releases/download/v0.121.0/eksctl_Linux_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          ls -la /usr/local/bin

    - run:
        name: "Check cluster info"
        command: |
          eksctl utils describe-stacks --region=us-west-1 --cluster=capstone-ci
          eksctl get iamidentitymapping --cluster capstone-ci --region=us-west-1
          eksctl delete --cluster capstone-ci --region=us-west-1
    - run:
        name: "Install kubectl"
        command: |
          # kubectl version --short

          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          chmod +x kubectl
          mkdir -p ~/.local/bin
          mv ./kubectl ~/.local/bin/kubectl
          ls -la ~/.local/bin/kubectl
          kubectl version --client --output=yaml
          aws eks update-kubeconfig --name capstone-ci --region us-west-1
          kubectl get nodes
          kubectl describe -n kube-system configmap/aws-auth
          aws sts get-caller-identity
          kubectl delete

    # - save_cache:
    #     key: eks-kube-dependency
    #     paths:
    #     - ~/project
    - persist_to_workspace:
        # Must be an absolute path, or relative path from working_directory. This is a directory on the container which is
        # taken to be the root directory of the workspace.
        root: ~/project
        # Must be relative path from root
        paths:
        - ./

  deployment-blue:
    docker:
    - image: cimg/aws:2022.11
    steps:
    - checkout

    - attach_workspace:
        at: ~/project
    # - restore_cache:
    #     keys:
    #     - eks-kube-dependency

    - run: aws eks update-kubeconfig --name capstone-ci --region us-west-1
    - run: kubectl describe configmap -n kube-system aws-auth
    - run: kubectl apply -f pod-flask.yaml
    # - run: kubectl create deploy flask-app --image=serglit72/flask-app:latest
    - run: kubectl get pods
      # - run: kubectl port-forward pod/flask-app-78c869bf94-chkh2 --address 0.0.0.0 5000:5050 | curl http://localhost:5000/status | grep "<h1>STATUS</h1>"




workflows:
  capstone-ms:
    jobs:
    # - build-env
    # - test-build:
    #     requires:
    #       - build-env
    # - docker-build-push:
    #     requires: 
    #       - test-build
    - create-vpc-for-kube-cluster
   
    # - infrastructure
    # - infrastructure-check
    # requires:
    # - infrastructure

    # - deployment-blue:
    #     requires:
    #     - infrastructure-check

    # - deployment-green:
    #     requires:
    #     - infrastructure

    # - test-blue:
    #     requires:
    #     - deployment-blue

    # - test-green:
    #     requires:
    #     - deployment-green

    # - switch-dns:
    #     requires:
    #     - test-green
    #     - test-blue

